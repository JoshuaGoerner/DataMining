{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import os\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5.1: News Article - Preprocessing\n",
    "\n",
    "\n",
    "In a first step, we are going to process news articles in order to try to apply clustering algorithms. The news article corpus-30docs includes 30 news articles from the domains of science, religion and politics (10 documents for each domain). Import the documents into your process and vectorize them using stopword removal and stemming (Porter). If necessary, transform all tokens to their lower cased version. How large are the resulting document vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 3027)\n"
     ]
    }
   ],
   "source": [
    "# Prep work\n",
    "data_dir = \"../data/\"\n",
    "doc_dirs = [\"sci.space\", \"soc.religion.christian\", \"talk.politics.guns\"]\n",
    "doc_paths = []\n",
    "\n",
    "# Get a list of all (relative) filepaths\n",
    "for d in doc_dirs:\n",
    "    for f in os.listdir(os.path.join(data_dir, d)):\n",
    "        doc_paths.append(os.path.join(data_dir, d, f))\n",
    "    \n",
    "# Create a custom Porter Stemmer that suits sklearn\n",
    "class PortStem(object):\n",
    "    def __init__(self):\n",
    "        self.ps = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.ps.stem(word) for word in word_tokenize(doc)]\n",
    "\n",
    "# Initiate a new vectorizer\n",
    "vectorizer = CountVectorizer(input=\"filename\",\n",
    "                             stop_words=\"english\", \n",
    "                             tokenizer=PortStem())\n",
    "\n",
    "# creating the bag of words\n",
    "bow = vectorizer.fit_transform(doc_paths)\n",
    "\n",
    "# evaluate num of features\n",
    "print(bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting Dataframe has **3027** attributes identified. Unfortunately the combination of `PorterStemmer()` and `word_tokenize` did not remove punctuation or numbers. Therefore a more advanced Tokenizer shall be used that:\n",
    "1. replaces punctuation and numbers to blanks\n",
    "2. stems based on the Porter algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a more advanced tokenizer\n",
    "class PortStemNoPunctNum(object):\n",
    "    def __init__(self):\n",
    "        self.ps = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        return [porter.stem(word)\n",
    "                for word \n",
    "                in word_tokenize(\n",
    "                doc.translate(\n",
    "                    str.maketrans(string.punctuation + \"0123456789\",' '*len(string.punctuation + \"0123456789\"))))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 2588)\n"
     ]
    }
   ],
   "source": [
    "# Initiate a new (advanced) vectorizer\n",
    "vectorizer_adv = CountVectorizer(input=\"filename\",\n",
    "                                 stop_words=\"english\",\n",
    "                                 tokenizer=PortStemNoPunctNum())\n",
    "\n",
    "# creating the bag of words\n",
    "bow_adv = vectorizer_adv.fit_transform(doc_paths)\n",
    "\n",
    "# evaluate num of features\n",
    "print(bow_adv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new Bag-of-Words only contains features without punctuation(`!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~`) and numbers. **The overall number of features is 2588 features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 5.2: News Article - Clustering\n",
    "Having imported the documents and transformed them into vectors, apply a k-means clustering using k = 3. How many documents ended up in the wrong cluster? What can you do to improve the clustering? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 5.3: News Article - Improved Clustering\n",
    "Does the distribution of frequent terms help you to further improve the clustering by using any of the prune methods of the Process Documents from File operator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 5.4: Job Postings: Preprocessing\n",
    "In a second step, we will focus on the classification of job postings. The jobpostings.xls  le contains 500 descriptions of job postings belonging to 30 different job categories like sales and real estate. Our main goal is to learn a classification model, which is capable to predict the correct category for a new job posting. Therefore, import the data into RapidMiner using the Read Excel and the Process Document from Data operators. Convert the textual description into a vector by applying tokenization and other preprocessing steps. In order to learn a good classi cation model, have a look at the generated attributes and basic setup for the preprocessing which removes noisy and misleading tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 5.5: Job Postings - Classification\n",
    "What levels of accuracy can you reach applying di erent classi cation methods and preprocessing settings?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
