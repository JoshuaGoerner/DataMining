{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 150 # helpful for itemsets\n",
    "from mlxtend.association import apriori, association_rules\n",
    "from collections import Counter\n",
    "\n",
    "# DATA IMPORT\n",
    "dt_basket = pd.read_excel(\"../data/ShoppingBaskets.xls\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\";>TO BE REFACTORED</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_associated_items(dt, item):\n",
    "    \"\"\"\"\"\"\n",
    "    itemcount_dict = Counter([item for itemsets in dt.values for item in itemsets])\n",
    "    itemcount_dict.pop(item)\n",
    "    return itemcount_dict\n",
    "\n",
    "def filter_frequent_itemsets(df_itemsets, items, exact=\"false\"):\n",
    "    pass\n",
    "\n",
    "def filter_rules_by_items(df_rules, items, criteria=\"all\"):\n",
    "    \"\"\"\"\"\"\n",
    "    df_tmp = df_rules.loc[:, [\"antecedants\", \"consequents\"]].applymap(lambda x: list(x))\n",
    "        \n",
    "    if criteria == \"all\":\n",
    "        supersets = np.array(df_tmp[\"antecedants\"] + df_tmp[\"consequents\"])\n",
    "    elif criteria == \"antecedants\":\n",
    "        supersets = np.array(df_tmp[\"antecedants\"])\n",
    "    elif criteria == \"consequents\":\n",
    "        supersets = np.array(df_tmp[\"consequents\"])\n",
    "    else:\n",
    "        raise ValueError(\"Criteria must be 'all', 'antecedants' or 'consequents', got{}\"\n",
    "                        .format(criteria))\n",
    "   \n",
    "    frozenset_vect = np.vectorize(lambda x: frozenset(x))\n",
    "    equality_vect = np.vectorize(lambda x,y: x == y)\n",
    "    return df_rules[equality_vect(frozenset_vect(supersets), set(items))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 4.1: Shopping Basket – Frequent Itemsets\n",
    "We want to analyse the buying behavior of our customers. The Shopping Baskets dataset describes the content of ten baskets, which are identified by the BasketNo attribute. The ten transactions altogether contain ten different items (products), while the corresponding attribute in the dataset states whether or not an item is included in a specific basket. Import the data into RapidMiner using the Read Excel operator. Please ensure that the attribute types and roles are set correctly. As a first task, we want to mine frequent itemsets using the FP-Growth operator with the parameter support set to 0.2 and the parameter positive value set of to 1. Which items are usually bought together with the laptop (ThinkPad X220), the netbook (Asus EeePC) and the printer (HP Laserjet P2055)?\n",
    "\n",
    "<span style=\"color:#AAAAAA;\">Comment: Since FP-Growth is not easily & reliably available, APRIORI is used with a minsup of 0.2</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThinkPad X220: \n",
      "Counter({'HP Laserjet P2055': 4, 'HP CE50 Toner': 4, 'Lenovo Tablet Sleeve': 4, '8 GB DDR3 RAM': 1, 'LT Laser Maus': 1})\n",
      "\n",
      "\n",
      "Asus EeePC: \n",
      "Counter({'Netbook-Schutzhülle ': 6, '2 GB DDR3 RAM': 6, 'LT Laser Maus': 4, 'LT Minimaus': 4})\n",
      "\n",
      "\n",
      "HP Laserjet P2055: \n",
      "Counter({'HP CE50 Toner': 4, 'ThinkPad X220 ': 4, 'Lenovo Tablet Sleeve': 4})\n"
     ]
    }
   ],
   "source": [
    "# Generate the frequent itemset\n",
    "dt_freq_basket = apriori(dt_basket, min_support=0.2, use_colnames=True)\n",
    "\n",
    "# Filter by ThinkPad X220\n",
    "mask_tp220 = dt_freq_basket[\"itemsets\"].map(lambda iset: \"ThinkPad X220 \" in iset) #! last char is [space]\n",
    "dt_freq_basket_tp220= dt_freq_basket[mask_tp220]\n",
    "print(\"ThinkPad X220: \\n{}\\n\\n\".format(get_associated_items(dt_freq_basket_tp220[\"itemsets\"], \"ThinkPad X220 \")))\n",
    "\n",
    "# Filter by Asus EeePC\n",
    "mask_aepc = dt_freq_basket[\"itemsets\"].map(lambda iset: \"Asus EeePC\" in iset)\n",
    "dt_freq_basket_aepc = dt_freq_basket[mask_aepc]\n",
    "print(\"Asus EeePC: \\n{}\\n\\n\".format(get_associated_items(dt_freq_basket_aepc[\"itemsets\"], \"Asus EeePC\")))\n",
    "\n",
    "# Filter by HP Laserjet P2055\n",
    "mask_hplj = dt_freq_basket[\"itemsets\"].map(lambda iset: \"HP Laserjet P2055\" in iset)\n",
    "dt_freq_basket_hplj = dt_freq_basket[mask_hplj]\n",
    "print(\"HP Laserjet P2055: \\n{}\".format(get_associated_items(dt_freq_basket_hplj[\"itemsets\"], \"HP Laserjet P2055\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 4.2: Shopping Basket – Association Rules\n",
    "What can the created rules based on the former created itemsets tell you about the relationship between Asus EeePC, 2 GB DDR3 RAM extensions and the Netbook-Schutzhülle? Try to judge the interestingness of the rules based on the lift values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#AAAAAA;\">Recap about lift</span>\n",
    "The _lift_ value is defined as $$ lift(X \\rightarrow Y) = \\frac{confidence(X \\rightarrow Y)}{\\sigma(Y)}$$\n",
    "where _confidence_ is defined as $$ confidence(X \\rightarrow Y) = \\frac{\\sigma(X \\cup Y)}{\\sigma(X)} $$\n",
    "and _$\\sigma$_ is the support.<br><br>\n",
    "\n",
    "Having this in mind, the lift values can be interpreted as a kind of directed correlation between the _ancedant_ and the _consequent_. The following cases can occur:\n",
    "- lift < 1 meaning that the rule $X \\rightarrow Y$ should rather be seen as $X \\rightarrow \\neg Y$\n",
    "- lift = 1 meaning that ancedant and consequent of the rule $X \\rightarrow $ are rather independent\n",
    "- lift > 1 meaning that the rule $X \\rightarrow Y$ should be definitively seen as $X \\overset{!}{\\rightarrow} Y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedants</th>\n",
       "      <th>consequents</th>\n",
       "      <th>support</th>\n",
       "      <th>lift</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Asus EeePC, Netbook-Schutzhülle )</td>\n",
       "      <td>(2 GB DDR3 RAM)</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Asus EeePC, 2 GB DDR3 RAM)</td>\n",
       "      <td>(Netbook-Schutzhülle )</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Netbook-Schutzhülle , 2 GB DDR3 RAM)</td>\n",
       "      <td>(Asus EeePC)</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Netbook-Schutzhülle )</td>\n",
       "      <td>(Asus EeePC, 2 GB DDR3 RAM)</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(2 GB DDR3 RAM)</td>\n",
       "      <td>(Asus EeePC, Netbook-Schutzhülle )</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             antecedants                         consequents  \\\n",
       "0     (Asus EeePC, Netbook-Schutzhülle )                     (2 GB DDR3 RAM)   \n",
       "1            (Asus EeePC, 2 GB DDR3 RAM)              (Netbook-Schutzhülle )   \n",
       "2  (Netbook-Schutzhülle , 2 GB DDR3 RAM)                        (Asus EeePC)   \n",
       "3                 (Netbook-Schutzhülle )         (Asus EeePC, 2 GB DDR3 RAM)   \n",
       "4                        (2 GB DDR3 RAM)  (Asus EeePC, Netbook-Schutzhülle )   \n",
       "\n",
       "   support      lift  confidence  \n",
       "0      0.4  2.000000         1.0  \n",
       "1      0.5  2.000000         0.8  \n",
       "2      0.4  1.666667         1.0  \n",
       "3      0.4  2.000000         1.0  \n",
       "4      0.5  2.000000         0.8  "
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_rules_by_items(df_rules, [\"Asus EeePC\", \"2 GB DDR3 RAM\", \"Netbook-Schutzhülle \"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all the lift values are greater than `1` none of the rules should be ommited and all rules can be considered \"interesting\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 4.3: Adult Dataset – Preprocessing\n",
    "In the following, we will work with a tweaked version (less attributes and less ex- amples, available as .ar  - le in the exercise repository) of this original dataset. In order to make it feasible for frequent itemset mining and association rule analysis, some preprocessing steps need to be performed. After you have read the dataset by using the Read ARFF operator, you  rst need to discretize the age and the hours- per-week attributes into three user-de ned ranges. Based on the original purpose of the dataset, think about what ranges might make sense. As frequent itemset mining only works on binary attributes, convert all attributes of the dataset into binary attributes.How many attributes does the dataset contain after executing the described preprocessing steps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 4.4: Adult Dataset – Frequent Itemsets\n",
    "What can you learn from these itemsets about the people who earn less than $50 000?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 4.5: Adult Dataset – Further Preprocessing\n",
    "From the resulting itemsets we can see, that, although we have a rather low min- imal support threshold, the variation of di erent educations is rather low. We can mostly observe itemsets including HS-grad and almost no other educations.\n",
    "Besides, also the native-country = United-States is dominant.\n",
    "Try to explain the dominance of those two attributes and think about a pos- sibility to aggregate the values further to reduce this dominance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 4.6: Adult Dataset – Finding Rich Americans\n",
    "Use the FP-Growth must contain parameter to restrict the patterns to the ones containing class => 50K and lower the support so that a decent number of item- sets is discovered. What can you learn from these itemsets about the people who earn more than $50 000 per year?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 4.7: Adult Dataset – Association Rule Mining\n",
    "As we have learned, looking only at the frequent itemsets might not reveal too many insights. Therefore, we now want to learn association rules. Apply the Create Association Rules operator to the process of the former task. Which rules do you consider to be interesting? Consider both => 50K and < 50K classes."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
